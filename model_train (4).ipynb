{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO9rq2eBnFPg"
      },
      "outputs": [],
      "source": [
        "# Import Stuff\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from multiprocessing import Process\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9FdobzjyHgT",
        "outputId": "f0b4c97e-1e10-47ed-e94c-2b7ad0938bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imgextract in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from imgextract) (1.17.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from imgextract) (0.19.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from imgextract) (9.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->imgextract) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->imgextract) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->imgextract) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->imgextract) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->imgextract) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->imgextract) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->imgextract) (23.2)\n"
          ]
        }
      ],
      "source": [
        "!pip  install imgextract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKyg8tLfnFP1"
      },
      "outputs": [],
      "source": [
        "# All Parameters required for training are declared over here\n",
        "# Frequency of Image Capturing\n",
        "FRAME_SKIP = 2\n",
        "# Frame Size\n",
        "FRAME_SIZE = (150,150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WEZvqPbEHto",
        "outputId": "b6b1a761-6ce9-4ed2-b31c-f4143cf6ecbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['git', 'clone', 'https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos'], returncode=0)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "# Remove the existing directory if it exists\n",
        "subprocess.run(['rm', '-rf', 'A-Dataset-for-Automatic-Violence-Detection-in-Videos/'])\n",
        "\n",
        "# Clone the repository\n",
        "subprocess.run(['git', 'clone', 'https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmh4-jdsEbTA"
      },
      "outputs": [],
      "source": [
        "!rm -r Data\n",
        "!mkdir Data\n",
        "!mkdir -p ./Data/Video/Violent\n",
        "!mkdir -p ./Data/Video/NonViolent\n",
        "!cp -a ./A-Dataset-for-Automatic-Violence-Detection-in-Videos/violence-detection-dataset/violent/cam1/. ./Data/Video/Violent/\n",
        "!cp -a ./A-Dataset-for-Automatic-Violence-Detection-in-Videos/violence-detection-dataset/non-violent/cam1/. ./Data/Video/NonViolent/\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "!mkdir -p ./Data/Training/V\n",
        "!mkdir -p ./Data/Training/NV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5s4Udk2FthH"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "class Extractor:\n",
        "    def __init__(self, frame_size, frame_skip):\n",
        "        self.frame_size = frame_size\n",
        "        self.frame_skip = frame_skip\n",
        "\n",
        "    def extract(self, video_path, category):\n",
        "        # Your frame extraction logic goes here\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frame_count = 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Your processing logic on each frame goes here\n",
        "            # For example, you can save the frame as an image file\n",
        "            frame_count += 1\n",
        "            frame_path = f\"./Data/Training/{category}/frame_{frame_count}.jpg\"\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "\n",
        "            # Skip frames based on the specified frame_skip\n",
        "            for _ in range(self.frame_skip - 1):\n",
        "                cap.read()\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD4dtKECEkxu"
      },
      "outputs": [],
      "source": [
        "def thread_1():\n",
        "    ext = Extractor(FRAME_SIZE, FRAME_SKIP)\n",
        "    for i in range(60):\n",
        "        path = f\"./Data/Video/Violent/{i+1}.mp4\"\n",
        "        print(f\"Processing Violent Vid-{i}\")\n",
        "        ext.extract(path, 'V')\n",
        "    print(\"Violent Extracted\")\n",
        "\n",
        "def thread_2():\n",
        "    ext = Extractor(FRAME_SIZE, FRAME_SKIP)\n",
        "    for i in range(60):\n",
        "        path = f\"/content/Data/Video/NonViolent/{i+1}.mp4\"\n",
        "        print(f\"Processing NonViolent Vid-{i}\")\n",
        "        ext.extract(path, 'NV')\n",
        "    print(\"Non-Violent Extracted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oNiKD1AEua8",
        "outputId": "17a25d43-4d41-41c1-997a-98fd1911ad87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Violent Vid-0\n",
            "Processing NonViolent Vid-0\n",
            "Processing Violent Vid-1\n",
            "Processing NonViolent Vid-1\n",
            "Processing NonViolent Vid-2\n",
            "Processing Violent Vid-2\n",
            "Processing NonViolent Vid-3\n",
            "Processing Violent Vid-3\n",
            "Processing NonViolent Vid-4\n",
            "Processing Violent Vid-4\n",
            "Processing Violent Vid-5\n",
            "Processing NonViolent Vid-5\n",
            "Processing NonViolent Vid-6\n",
            "Processing Violent Vid-6\n",
            "Processing NonViolent Vid-7\n",
            "Processing Violent Vid-7\n",
            "Processing NonViolent Vid-8\n",
            "Processing Violent Vid-8\n",
            "Processing NonViolent Vid-9\n",
            "Processing Violent Vid-9\n",
            "Processing NonViolent Vid-10\n",
            "Processing NonViolent Vid-11\n",
            "Processing Violent Vid-10\n",
            "Processing NonViolent Vid-12\n",
            "Processing Violent Vid-11\n",
            "Processing Violent Vid-12\n",
            "Processing NonViolent Vid-13\n",
            "Processing Violent Vid-13\n",
            "Processing NonViolent Vid-14\n",
            "Processing Violent Vid-14\n",
            "Processing Violent Vid-15\n",
            "Processing NonViolent Vid-15\n",
            "Processing NonViolent Vid-16\n",
            "Processing Violent Vid-16\n",
            "Processing Violent Vid-17\n",
            "Processing NonViolent Vid-17\n",
            "Processing Violent Vid-18\n",
            "Processing NonViolent Vid-18\n",
            "Processing Violent Vid-19\n",
            "Processing NonViolent Vid-19\n",
            "Processing Violent Vid-20\n",
            "Processing NonViolent Vid-20\n",
            "Processing Violent Vid-21\n",
            "Processing NonViolent Vid-21\n",
            "Processing Violent Vid-22\n",
            "Processing NonViolent Vid-22\n",
            "Processing Violent Vid-23\n",
            "Processing NonViolent Vid-23\n",
            "Processing Violent Vid-24\n",
            "Processing NonViolent Vid-24\n",
            "Processing Violent Vid-25\n",
            "Processing NonViolent Vid-25\n",
            "Processing Violent Vid-26\n",
            "Processing NonViolent Vid-26\n",
            "Processing Violent Vid-27\n",
            "Processing NonViolent Vid-27\n",
            "Processing Violent Vid-28\n",
            "Processing NonViolent Vid-28\n",
            "Processing Violent Vid-29\n",
            "Processing NonViolent Vid-29\n",
            "Processing Violent Vid-30\n",
            "Processing NonViolent Vid-30\n",
            "Processing Violent Vid-31\n",
            "Processing Violent Vid-32\n",
            "Processing NonViolent Vid-31\n",
            "Processing Violent Vid-33\n",
            "Processing NonViolent Vid-32\n",
            "Processing Violent Vid-34\n",
            "Processing NonViolent Vid-33\n",
            "Processing NonViolent Vid-34\n",
            "Processing Violent Vid-35\n",
            "Processing NonViolent Vid-35\n",
            "Processing Violent Vid-36\n",
            "Processing NonViolent Vid-36\n",
            "Processing NonViolent Vid-37\n",
            "Processing Violent Vid-37\n",
            "Processing NonViolent Vid-38\n",
            "Processing NonViolent Vid-39\n",
            "Processing Violent Vid-38\n",
            "Processing Violent Vid-39\n",
            "Processing NonViolent Vid-40\n",
            "Processing Violent Vid-40\n",
            "Processing NonViolent Vid-41\n",
            "Processing Violent Vid-41\n",
            "Processing NonViolent Vid-42\n",
            "Processing NonViolent Vid-43\n",
            "Processing Violent Vid-42\n",
            "Processing NonViolent Vid-44\n",
            "Processing NonViolent Vid-45\n",
            "Processing Violent Vid-43\n",
            "Processing NonViolent Vid-46\n",
            "Processing Violent Vid-44\n",
            "Processing NonViolent Vid-47\n",
            "Processing Violent Vid-45\n",
            "Processing NonViolent Vid-48\n",
            "Processing Violent Vid-46\n",
            "Processing NonViolent Vid-49\n",
            "Processing Violent Vid-47\n",
            "Processing NonViolent Vid-50\n",
            "Processing Violent Vid-48\n",
            "Processing NonViolent Vid-51\n",
            "Processing NonViolent Vid-52\n",
            "Processing Violent Vid-49\n",
            "Processing NonViolent Vid-53\n",
            "Processing Violent Vid-50\n",
            "Processing NonViolent Vid-54\n",
            "Processing NonViolent Vid-55\n",
            "Processing Violent Vid-51\n",
            "Processing Violent Vid-52\n",
            "Processing NonViolent Vid-56\n",
            "Processing Violent Vid-53\n",
            "Processing NonViolent Vid-57\n",
            "Processing NonViolent Vid-58\n",
            "Processing Violent Vid-54\n",
            "Processing NonViolent Vid-59\n",
            "Processing Violent Vid-55\n",
            "Non-Violent Extracted\n",
            "Processing Violent Vid-56\n",
            "Processing Violent Vid-57\n",
            "Processing Violent Vid-58\n",
            "Processing Violent Vid-59\n",
            "Violent Extracted\n",
            "Complete\n"
          ]
        }
      ],
      "source": [
        "# Violent Extraction\n",
        "from multiprocessing import Process\n",
        "\n",
        "# Your other imports and code here\n",
        "\n",
        "t1 = Process(target=thread_1, args=())\n",
        "t2 = Process(target=thread_2, args=())\n",
        "\n",
        "t1.start()\n",
        "t2.start()\n",
        "# NonViolent Extraction\n",
        "\n",
        "t1.join()\n",
        "t2.join()\n",
        "print(\"Complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjSO8xyLlBM2"
      },
      "outputs": [],
      "source": [
        "base_dir='./Data'\n",
        "train_dir=os.path.join(base_dir,'Training')\n",
        "train_violent_dir =os.path.join(train_dir, 'V' )\n",
        "train_nonviolent_dir=os.path.join(train_dir,'NV')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IMpePFlfqft"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjTCcUqmDDkN"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest',\n",
        "                                   validation_split=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u95ZhHxoMWgQ",
        "outputId": "97874d63-c26f-4391-d156-f1111b2062bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 302 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    color_mode=\"rgb\",\n",
        "    target_size=FRAME_SIZE,\n",
        "    batch_size=20,\n",
        "    classes=['NV', 'V'],\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    subset='training'  # This will select the training subset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3PfEP9KMa6V",
        "outputId": "7985baff-78e0-4545-d38b-bb8132fd2304"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 100 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    color_mode=\"rgb\",\n",
        "    target_size=FRAME_SIZE,\n",
        "    batch_size=20,\n",
        "    classes=['NV', 'V'],\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    subset='validation'  # This will select the validation subset\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrIkihfOmcbS"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "steps_per_epoch = math.ceil(train_generator.samples / train_generator.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk8zYR_pt-6w"
      },
      "outputs": [],
      "source": [
        "model= tf.keras.models.Sequential([\n",
        "       tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)),\n",
        "       tf.keras.layers.MaxPooling2D(2,2),\n",
        "       tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "       tf.keras.layers.MaxPooling2D(2,2),\n",
        "       tf.keras.layers.Conv2D(128,(3,3),activation='relu'),\n",
        "       tf.keras.layers.MaxPooling2D(2,2),\n",
        "       tf.keras.layers.Conv2D(128,(3,3),activation='relu'),\n",
        "       tf.keras.layers.MaxPooling2D(2,2),\n",
        "       tf.keras.layers.Dropout(0.5),\n",
        "       tf.keras.layers.Flatten(),\n",
        "       tf.keras.layers.Dense(512, activation='relu'),\n",
        "       tf.keras.layers.Dense(1,activation ='sigmoid')\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idaPR2xmuuLs",
        "outputId": "37e0edef-dafe-4518-9de8-527d18cf7040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7373 - accuracy: 0.4834"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r16/16 [==============================] - 10s 504ms/step - loss: 0.7373 - accuracy: 0.4834 - val_loss: 0.6889 - val_accuracy: 0.6700\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 0.6882 - accuracy: 0.5497\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - 5s 322ms/step - loss: 0.6314 - accuracy: 0.6391\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - 5s 333ms/step - loss: 0.5705 - accuracy: 0.6854\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.3693 - accuracy: 0.8311\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch ,\n",
        "    epochs=5,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEmQZDqgmmg3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5rcrGFt26_B"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation accuracy values\n",
        "history.plot(metric='accuracy')\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "history.plot(metric='loss')\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lth_pTIqlPHY"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(validation_generator)\n",
        "print(\"Testing Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q446tFFu9pD"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "t = time.time()\n",
        "export_path_keras = \"./{}.h5\".format(int(t))\n",
        "print(export_path_keras)\n",
        "\n",
        "model.save(export_path_keras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3fSDpTen4uG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Define your desired model name here\n",
        "model_name = \"CNN\"\n",
        "\n",
        "# Define the export path with the custom model name\n",
        "export_path_keras = f\"./{model_name}.h5\"\n",
        "\n",
        "# Save the model with the specified export path\n",
        "model.save(export_path_keras)\n",
        "\n",
        "print(\"Model saved as:\", export_path_keras)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
